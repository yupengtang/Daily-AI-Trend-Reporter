---
layout: post
title: "Daily AI Research Papers - Saturday, February 08, 2025"
date: 2025-02-08
---

**ðŸ”‘ Keywords**: feature flow, language models, AlphaGeometry2, interpretable features, AI oversight, omni-modal models, dynamic content augmentation, speech synthesis, chain-of-thought reasoning, instruction following, diffusion transformers, modality alignment

**1. Analyze Feature Flow to Enhance Interpretation and Steering in Language
  Models**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.03032)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**2. Gold-medalist Performance in Solving Olympiad Geometry with
  AlphaGeometry2**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.03544)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**3. ConceptAttention: Diffusion Transformers Learn Highly Interpretable
  Features**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.04320)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**4. Great Models Think Alike and this Undermines AI Oversight**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.04313)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**5. Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive
  Modality Alignment**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.04328)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**6. DynVFX: Augmenting Real Videos with Dynamic Content**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.03621)  
ðŸ“‹ Summary: \r\nWe present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time. The position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene, resulting in a cohesive and realistic output video. We achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail. Specifically, we introduce a novel inference-based method that manipulates features within the attention mechanism, enabling accurate localization and seamless integration of the new content while preserving the integrity of the original scene. Our method is fully automated, requiring only a simple user instruction. We demonstrate its effectiveness on a wide range of edits applied to real-world videos, encompassing diverse objects and scenarios involving both camera and object motion.\r\n\r\nProject page: \r\nhttps://dynvfx.github.io/&quot;,&quot;html&quot;:&quot;

**7. Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based
  Speech Synthesis**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.04128)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**8. BOLT: Bootstrap Long Chain-of-Thought in Language Models without
  Distillation**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.03860)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**9. UltraIF: Advancing Instruction Following from the Wild**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.04153)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**10. Weak-to-Strong Diffusion with Reflection**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.00473)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">
