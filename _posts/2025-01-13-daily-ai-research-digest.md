---
layout: post
title: "Daily AI Research Papers - January 13, 2025"
date: 2025-01-13
---

**ðŸ”‘ Keywords**: self-evolving critic, retrieval-augmented generation, visual reasoning, robotic manipulation, video understanding, multimodal models, multiagent finetuning, structured image understanding, video customization, diffusion transformers, personalization, spatial constraints

**1. Enabling Scalable Oversight via Self-Evolving Critic**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.05727)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**2. VideoRAG: Retrieval-Augmented Generation over Video Corpus**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.05874)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**3. LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.06186)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**4. OmniManip: Towards General Robotic Manipulation via Object-Centric
  Interaction Primitives as Spatial Constraints**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.03841)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**5. OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video
  Understanding?**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.05510)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**6. Migician: Revealing the Magic of Free-Form Multi-Image Grounding in
  Multimodal Large Language Models**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.05767)  
ðŸ“‹ Summary: 30,&quot;discussionId&quot;:&quot;67850073609833baf1d0c9b2&quot;,&quot;ai_summary&quot;:&quot;Migician, a multi-image grounding model, outperforms existing MLLMs in complex multi-image scenarios using the MGrounding-630k dataset and MIG-Bench benchmark.&quot;,&quot;ai_keywords&quot;:[&quot;Multimodal Large Language Models&quot;,&quot;Chain-of-Thought&quot;,&quot;Migician&quot;,&quot;multi-image grounding&quot;,&quot;MGrounding-630k dataset&quot;,&quot;MIG-Bench&quot;,&quot;free-form grounding&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;642086ed290342c5df85662d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/915a4d7b89455ae97b8544c79286ddf8.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chi Chen&quot;,&quot;user&quot;:&quot;carboncoo&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;669a0be38f2dbc203fb8ba8c&quot;,&quot;avatarUrl&quot;:&quot;/avatars/603ff6e812a73451c77aece0b54e6274.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;heyu&quot;,&quot;user&quot;:&quot;Caffiener&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;654f3e104c8874c64d43aafa&quot;,&quot;avatarUrl&quot;:&quot;/avatars/00de263f98a81c52cdb321fb11b16c06.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;You Li&quot;,&quot;user&quot;:&quot;Michael4933&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67130ccc6affd40356918480&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NHWozbFlQajTxgiEIhOMI.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;yuan&quot;,&quot;user&quot;:&quot;long1235&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;661691fbcf576737335e1259&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e811adb3e40dcc9187ec3c8af994851b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;xugenrui&quot;,&quot;user&quot;:&quot;xugenrui&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;648183b3ec65b8b77d84693a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a3474e8839771a084e88522c6985d6db.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shuo Wang&quot;,&quot;user&quot;:&quot;shuo-hf&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6491af36c1741666238f3bff&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0ee7d2ec1566e2cc5e8f144140e17f00.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zonghao Guo&quot;,&quot;user&quot;:&quot;guozonghao96&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;660aa56b55ba149ec17d326e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/960ecda86ad3c02df61bfd31f767096b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Donald King&quot;,&quot;user&quot;:&quot;DonaldJKing&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6443883a5af87c73bbb75825&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7d88e2968a86923e41782190242840ce.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;LostSpirit&quot;,&quot;user&quot;:&quot;LostSpirit1307&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6785c5de58328c47557dcd94&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QgWM1f07Srpea8ySGDVaK.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;one&quot;,&quot;user&quot;:&quot;bigcg&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63c3b67ec7d7f4c63a4eea3a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4a5f98cb6b0c1e37a2c09af72f7a9946.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinyu Ma&quot;,&quot;user&quot;:&quot;MaxyLee&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6638533736f6e89578b4e99d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/6bc86695b071c4a7a929d99e0c50a076.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sun&quot;,&quot;user&quot;:&quot;sunmk&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;*&quot;],&quot;dailyPaperRank&quot;:0}">

**7. Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.05707)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**8. ReFocus: Visual Editing as a Chain of Thought for Structured Image
  Understanding**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.05452)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**9. ConceptMaster: Multi-Concept Video Customization on Diffusion
  Transformer Models Without Test-Time Tuning**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.04698)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**10. Multi-subject Open-set Personalization in Video Generation**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2501.06187)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">
