---
layout: post
title: "Daily AI Research Papers - Thursday, February 06, 2025"
date: 2025-02-06
---

**ðŸ”‘ Keywords**: language models, reasoning, data-centric training, multimodal reasoning, financial simulation, cognitive alignment, code generation, probabilistic inference, Monte Carlo methods, distillation, structured thinking, behavioral simulation

**1. SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language
  Model**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.02737)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**2. LIMO: Less is More for Reasoning**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.03387)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**3. Demystifying Long Chain-of-Thought Reasoning in LLMs**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.03373)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**4. TwinMarket: A Scalable Behavioral and Social Simulation for Financial
  Markets**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.01506)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**5. Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.02339)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**6. LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion
  Transformer**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.01105)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**7. Token Assorted: Mixing Latent and Text Tokens for Improved Language
  Model Reasoning**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.03275)  
ðŸ“‹ Summary: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks.&quot;,&quot;upvotes&quot;:18,&quot;discussionId&quot;:&quot;67a448b89ca42c642a723ac6&quot;,&quot;ai_summary&quot;:&quot;A hybrid representation using latent tokens generated by VQ-VAE reduces the length of reasoning traces in large language models, improving performance on both reasoning and fine-tuning tasks.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Models (LLMs)&quot;,&quot;chain-of-thought (CoT)&quot;,&quot;VQ-VAE&quot;,&quot;latent discrete tokens&quot;,&quot;Keys-Finding Maze problem&quot;,&quot;logical reasoning&quot;,&quot;mathematical reasoning&quot;,&quot;mixed training procedure&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;620783f24e28382272337ba4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;GuoLiangTang&quot;,&quot;user&quot;:&quot;Tommy930&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;648eb1eb59c4e5c87dc116e0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c636cea39c2c0937f01398c94ead5dad.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;fdsqefsgergd&quot;,&quot;user&quot;:&quot;T-representer&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63e5beba006a775275e79665&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3be5d4df71b81c9e5a2255a507f73542.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Weijing Huang&quot;,&quot;user&quot;:&quot;waleking&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;5e56829137cb5b49818287ea&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/5e56829137cb5b49818287ea/8HYzJeRc4b9Wu7BfJwibS.png&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Lee Junbum&quot;,&quot;user&quot;:&quot;beomi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63082bb7bc0a2a5ee2253523&quot;,&quot;avatarUrl&quot;:&quot;/avatars/6cf8d12d16d15db1070fbea89b5b3967.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kuo-Hsin Tu&quot;,&quot;user&quot;:&quot;dapumptu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64137e2150358a805203cbac&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64137e2150358a805203cbac/w9RQx8Q07UvgFyIZ3ce_k.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jade&quot;,&quot;user&quot;:&quot;euclaise&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65839aacd9ea8286ded1c7ba&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c93a6f00be5775c57cd4fbde301c6e35.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;abadir &quot;,&quot;user&quot;:&quot;Aonei&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;647daf00cfca67bc50f9a99f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chenhao(Leo) Zhang&quot;,&quot;user&quot;:&quot;MING-ZCH&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;649be88f867d442094248239&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NFwa74mbWNEBfHlt82hpp.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;SAMBIT CHAKRABORTY&quot;,&quot;user&quot;:&quot;sambitchakhf03&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;5f43448a79c1ba4c353d0d8f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/5f43448a79c1ba4c353d0d8f/DiSygV3dn7A_OjmGVTrHD.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Sugato Ray&quot;,&quot;user&quot;:&quot;sugatoray&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;668cd4bbe990292e5f6974d3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d1747b2372e94500ecb5fb56809b482d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jinyeong Kim&quot;,&quot;user&quot;:&quot;rubatoyeong&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66f612b934b8ac9ffa44f084&quot;,&quot;avatarUrl&quot;:&quot;/avatars/6836c122e19c66c90f1673f28b30d7f0.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tang&quot;,&quot;user&quot;:&quot;tommysally&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;*&quot;],&quot;dailyPaperRank&quot;:0}">

**8. On Teacher Hacking in Language Model Distillation**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.02671)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**9. Large Language Model Guided Self-Debugging Code Generation**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.02928)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">

**10. A Probabilistic Inference Approach to Inference-Time Scaling of LLMs
  using Particle-Based Monte Carlo Methods**  
ðŸ”— [Read Paper](https://huggingface.co/papers/2502.01618)  
ðŸ“‹ Summary: border-blue-800 dark:bg-blue-900/15">
